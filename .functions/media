####################################
# Get Media from Online
####################################

get_song_from_youtube() {
      local SITE=${1}
      local LOCATION=$(pwd)
      cd ~/Music
      youtube-dl --audio-format mp3 -x ${SITE}
      cd ${LOCATION}
  }

alias getsong=get_song_from_youtube


# Grab a compressed file from online and uncompress it
curltar() {
        case $1 in
                *.tar.bz2) \curl -kL $1 | tar xvjf - ;;
                *.tar.gz) \curl -kL $1 | tar xvzf - ;;
                *.bz2) \curl -kL $1 | bunzip2 - ;;
                *.rar) \curl -kL $1 | unrar x - ;;
                *.gz) \curl -kL $1 | gunzip - ;;
                *.tar) \curl -kL $1 | tar xvf - ;;
                *.tbz2) \curl -kL $1 | tar xvjf - ;;
                *.tgz) \curl -kL $1 | tar xvzf - ;;
                *.zip) \curl -kL $1 | unzip - ;;
                *.Z) \curl -kL $1 | uncompress - ;;
                *.7z) \curl -kL $1 | 7z x - ;;
                *) \curl -kLO $1
        esac
}

curl_w_resume() {
    # From the following one liner
    # export ec=18; while [ $ec -eq 18 ]; do /usr/bin/curl -O -C - "http://..........zip"; export ec=$?; done
    local url="${1}"
    local ec=18;
    while [ $ec -eq 18 ]; do
        curl -O -C - "${url}";
        ec=$?;
    done
}



# Evidence Collection

get_evidence_wget_from_url() {
    local url="$1"
    local urlid=$(echo "$url" | tr -cd '[:alnum:]._-')
    local urlid="${urlid:0:10}-$(uuidgen)"
    mkdir "${urlid}"
    cd "${urlid}"
    wget_mirror_single_page \
             --no-check-certificate \
             -U "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" \
             "${url}"
    cd ..
    echo "Stored at ${urlid}"
}

get_evidence_youtube_from_url() {
    local url="$1"
    local urlid=$(echo "$url" | tr -cd '[:alnum:]._-')
    local urlid="${urlid:0:10}-$(uuidgen)"
    mkdir "${urlid}"
    cd "${urlid}"
    get_youtube_evidence --no-check-certificate "${url}"
    cd ..
    echo "Stored at ${urlid}"
}

get_youtube_evidence() {
    # Downloads Youtube videos as evidence
    youtube-dl -o '%(id)s-%(title)s-%(uploader)s-%(upload_date)s.%(ext)s' \
               --write-description \
               --write-annotations \
               --write-all-thumbnails \
               --all-subs  --write-sub --write-auto-sub \
               --write-info-json \
               $@
}

####################################
# Read Media
####################################

decode_qr() {
    # decode a QR code from an image
    # sudo apt-get install zbar-tools
    zbarimg "${1}"
}


markdown_quick_view() {
    pandoc -s -f markdown -t man "${1}" | man -l -
}


####################################
# Convert and Format Media
####################################

# Create a data URL from a file
function dataurl_create() {
        local mimeType=$(file -b --mime-type "$1");
        if [[ $mimeType == text/* ]]; then
                mimeType="${mimeType};charset=utf-8";
        fi
        echo "data:${mimeType};base64,$(openssl base64 -in "$1" | tr -d '\n')";
}



rip_dvd() {
    local VOB_PATH="$1" # VTS_01_1.VOB
    local AVI_PATH="$2" # ~/video_01_1
    # -f avi : force output format
    # -c:v libx264 : encode with the h264 codec to create small, but quality files
    # -g 300 : GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video
    avconv -i "${VOB_PATH}" -f avi -c:v libx264 -g 300 -bf 2 "${AVI_PATH}".avi

    # Note for mounting dvd's
    # sudo mount -o loop /dev/cdrom /media/cdrom/
}

alias vob_to_avi="rip_dvd"


# Latex
inkscape2latex() {
    inkscape -D -z --file=${1}.svg --export-pdf=${1}.pdf --export-latex
}


# Whiteboard Cleaning
white-board-cleaner () {
    convert "$1" -morphology Convolve DoG:15,100,0 -negate -normalize -blur 0x1 -channel RBG -level 60%,91%,0.1 "$2" ;
}


# This is just a stub that can be modified to work better
# with different photos
annotate() {
    # annotate [image_name] "[annotation]"
    # produces text_[image_name] file
    # ONLY use this in the same directory as the image.

    local image_name=$1
    local annotation=$2

    # North, South, East, West
    # NorthEast, SouthWest, Etc.
    local gravity="East"
    # Font Selection
    # See all with following command
    # convert -list font | grep "Font:"
    local font="Droid-Sans"
    local pointsize=25
    # Fold the text at a certain char
    # change this to make the text wrap
    local foldchar=15
    # This makes it only fold on spaces
    local spacefold="-s"
    # This would make it not fold.
    # local spacefold=""

    convert -gravity $gravity \
            -weight 800 \
            -font $font -pointsize $pointsize \
            -annotate +20+20 "$(fold -w ${foldchar} ${spacefold} <<< "${annotation}")" \
            "$image_name" text_"$image_name"
}


pdf_remove_password() {
  # Requires xpdf-utils
  local filename="$1"
  local password="$2"
  # pdftk input.pdf output output.pdf user_pw YOURPASSWORD-HERE
  # pdftk input.pdf output output.pdf user_pw YOURPASSWORD-HERE owner_pw YOURPASSWORD-HERE
  # pdftk input.pdf output output.pdf input_pw YOURPASSWORD-HERE
  pdftops -upw "${password}" "${filename}"
  ps2pdf "${filename/%.pdf/.ps}" "${filename/%.pdf/_no_password.pdf}"
}



####################################
# Bulk Download Media from Webpage
####################################


scrape_files_from_url() {
    local url="$1"
    local filetype="$2"
    local is_verify_off="$3"
    if [[ "${is_verify_off}" == "NO_VERIFY" ]]; then
        verify_flag="--no-check-certificate"
    else
        verify_flag="-i"
    fi
    IFS=$'\n'
    for item in $(python3 ~/dotfiles/bin/scrape_links_with_text_by_filetype.py -u "$url" -f "$filetype" "$verify_flag"); do
        local fileurl=$(echo "$item" | sed 's/\(.*\)::::\(.*\)/\1/g')
        local filename=$(echo "$item" | sed 's/\(.*\)::::\(.*\)/\2/g' | sed 's/[[:punct:]]//g' | sed 's/[[:space:]]/_/g')
        local extension=$(echo "$item" | sed 's/\(.*\)::::\(.*\)/\1/g'| rev | cut -d. -f1 | rev)
        #echo "$item"
        #echo $fileurl
        #echo "${filename}.${extension}"
        wget "$verify_flag" -O "${filename}.${extension}" "${fileurl}"
     done
}

alias download_files_from_url="scrape_files_from_url"

scrape_files_from_url_no_rename() {
    local url="$1"
    local filetype="$2"
    local is_verify_off="$3"
    if [[ "${is_verify_off}" == "NO_VERIFY" ]]; then
        verify_flag="--no-check-certificate"
    else
        verify_flag=""
    fi
    wget -r -np -nd -l 1 -A "$filetype" "$verify_flag" "${url}"
}


download_open_directory() {
    #set -x;
    regex='^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\?([^#]*))?(#(.*))?'

    if [[ $1 =~ $regex ]]; then
        dirs=${BASH_REMATCH[5]}
        trimmed="${dirs:1:${#dirs}-2}"
        slashes="${trimmed//[^\/]}"
        count="${#slashes}"
        wget --no-check-certificate -q --show-progress -r -np -nH --cut-dirs=${count} -R "index.html*" "$1/"
    else
        echo "URL not understood."
    fi
    #set +x;
}

alias wget_wget_download_open_directory=wget_download_open_directory
